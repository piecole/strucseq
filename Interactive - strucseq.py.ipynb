{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from Bio.PDB import *\n",
    "from Bio.Blast import NCBIWWW\n",
    "from Bio.Blast import NCBIXML\n",
    "from typing import Union\n",
    "import ast\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import csv\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import propka.run as pk\n",
    "except:\n",
    "    print(\"PROPKA not installed, will not be able to determine pka through PROPKA.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    from rcsbsearchapi.search import TextQuery as PDBquery\n",
    "except:\n",
    "    print(\"rcsbsearchapi not installed, some functions may not work.\")\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas()\n",
    "except:\n",
    "    def tqdm(iterator, *args, **kwargs):\n",
    "        return iterator\n",
    "    print(\"tqdm not installed, progress bars will not work.\")\n",
    "    \n",
    "alphabet = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\", \"DCY\"]\n",
    "threeletter = [\"ALA\",\"DCY\",\"CYS\",\"ASP\",\"GLU\",\"PHE\",\"GLY\",\"HIS\",\"ILE\",\"JXX\",\"LYS\",\"LEU\",\"MET\",\"ASN\",\"OXX\",\"PRO\",\"GLN\",\"ARG\",\"SER\",\"THR\",\"MSE\",\"VAL\",\"TRP\",\"TPO\",\"TYR\",\"SEP\"]\n",
    "threetoone = dict(zip(threeletter, alphabet))\n",
    "\n",
    "# Fix forward slashes in filepaths and create missing folders\n",
    "def parse_folder(input_folder : str):\n",
    "    assert isinstance(input_folder, str), \"str expected for input_folder, got\" + repr(type(input_folder))\n",
    "    input_folder = input_folder.replace(\"\\\\\", \"/\")\n",
    "    if input_folder[-1] != \"/\":\n",
    "        input_folder = input_folder + \"/\"\n",
    "    \n",
    "    # Create missing folder\n",
    "    if not os.path.exists(input_folder):\n",
    "        os.makedirs(input_folder)\n",
    "\n",
    "    return input_folder\n",
    "\n",
    "get_pKa = { #this dictioary defines the pKas of amino acid side chains\n",
    "    #according to: https://www.sigmaaldrich.com/life-science/metabolomics/learning-center/amino-acid-reference-chart.html#prop\n",
    "    \"R\": 12.48, \"D\": 3.65, \"C\": 8.18, \"E\": 4.25, \"H\": 6.00, \"K\": 10.53, \"Y\": 10.07, \"U\":1, \"X\":5.9, \"Z\":5.6, \"DCY\":8.18\n",
    "    #selenomethionine \"U\" has been given as pKa 1 as no value is known\n",
    "    #phosphoserine and phospohthreonine are from 16018962\n",
    "    #R-cysteine given same value as L-cysteine\n",
    "}\n",
    "get_charge = {\n",
    "    \"R\": 1, \"D\": -1, \"E\": -1, \"H\": 1, \"K\": 1\n",
    "    }\n",
    "\n",
    "get_hydrophobic7 = { #all the amino acid hydrophobicities at pH7\n",
    "    \"A\":41,\n",
    "    \"C\":49,\n",
    "    \"D\":-55,\n",
    "    \"E\":31,\n",
    "    \"F\":97,\n",
    "    \"G\":0,\n",
    "    \"H\":8,\n",
    "    \"I\":99,\n",
    "    \"K\":-23,\n",
    "    \"L\":97,\n",
    "    \"M\":74,\n",
    "    \"N\":-28,\n",
    "    \"P\":-49, #reference uses pH2 value for some reason\n",
    "    \"Q\":-10,\n",
    "    \"R\":-14,\n",
    "    \"S\":-5,\n",
    "    \"T\":13,\n",
    "    \"U\":74, #selenomethonine same as methionine since no value is known for selenomethionine? - check this\n",
    "    \"V\":76,\n",
    "    \"W\":97,\n",
    "    \"X\":0, #phospohthreonine unknown what value to give\n",
    "    \"Y\":63,\n",
    "    \"Z\":0, #phosphothreonine unknown what value to give\n",
    "    \"DCY\":49 #R-cysteine same as L cysteine\n",
    "}\n",
    "\n",
    "def get_uniprot_accessions(pdbcode : str, strict = True, selenium = False, debug = False) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a pdb assession code and accesses the PDB to get the Uniprot assession \n",
    "    code of the protein for each chain\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdbcode : str\n",
    "        The 4-letter protein data bank (PDB) code.\n",
    "    strict : bool, optional\n",
    "        If False, will not raise an exception in input is incorrect.\n",
    "    selenium : bool, optional\n",
    "        Choose to use selenium to scrabe the browser version, rather than the XML \n",
    "        version (currently not got this feature working)\n",
    "    debug : bool, optional\n",
    "        Write information to console\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Uniprot accession codes of each chain in the strcture. Key is the chain identifier,\n",
    "        content is the accession code.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #   Catch incorrect inputs as long as strict is True\n",
    "    if strict == True:\n",
    "        assert isinstance(pdbcode, str) == True, \"Expected 4 letter string for pdbcode, got type: \" + type(pdbcode).__name__ + \".\"\n",
    "        assert len(pdbcode) == 4, f\"Expected 4 letter string for pdbcode, got: '{pdbcode}'.\"\n",
    "    \n",
    "    #   Fetch the xml version of the uniprot site in beautifulsoup\n",
    "    failed = False\n",
    "    if selenium == False:\n",
    "        fails = 0\n",
    "        worked = False\n",
    "        while worked == False:\n",
    "            try:\n",
    "                if debug == True:\n",
    "                    print(f\"Getting PDB XML for {pdbcode}\")\n",
    "                soup = BeautifulSoup(requests.get(\"https://files.rcsb.org/view/\" + pdbcode + \"-noatom.xml\").text, \"lxml\")\n",
    "                worked = True\n",
    "            except:\n",
    "                fails = fails + 1\n",
    "                time.sleep(fails**2)\n",
    "        \n",
    "        entries = {} #  Create a dictionary to store each entry\n",
    "        failed = False\n",
    "        try:\n",
    "            for entry in soup.find(\"pdbx:struct_ref_seqcategory\").find_all(\"pdbx:struct_ref_seq\"): #find each chain entry\n",
    "                try:\n",
    "                    info = entry.find(\"pdbx:pdbx_db_accession\").text\n",
    "                    if len(info) > 4:\n",
    "                        entries[entry.find(\"pdbx:pdbx_strand_id\").text] = info #extract each one as a uniprot code assigend with a chain letter\n",
    "                except:\n",
    "                    if debug == True:\n",
    "                        print(\"no uniprot accession found\")\n",
    "        except:\n",
    "            if debug == True:\n",
    "                print(\"no accession section found for \", pdbcode)\n",
    "            failed = True\n",
    "            \n",
    "            for entry in entries:\n",
    "                if len(entries[entry]) > 6: #   Check if entry is wrong length, then what?\n",
    "                    failed = True\n",
    "                    if debug == True:\n",
    "                        print(\"Bad uniprots for \" + pdbcode)\n",
    "                    break\n",
    "            \n",
    "    return entries\n",
    "\n",
    "\n",
    "#   OLD VERSION OF THE FUNCTION:\n",
    "def iterate_uniprot_accessions_OLD(in_csv : str, chain_cols : list, out_csv : str, delimiter = \"\\t\", rows : str = 10000000000000, debug = True):\n",
    "    \"\"\"\n",
    "    Takes an input CSV with a PDBid header and some custom headers to get the chains from\n",
    "    and returns a csv with the assession codes and unique structures and chains.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_csv : str\n",
    "        The the input CSV with PDBids that is used to generate accession codes.\n",
    "    out_csv : str\n",
    "        The output CSV used to store fetched accession codes.\n",
    "    delimiter : str, optional\n",
    "        Delimiter of the CSV file\n",
    "    rows : str, optional\n",
    "        The max index for fetching, usually for testing. The default is 10000000000000.\n",
    "    debug : bool, optional\n",
    "        Whether to print progess or other notifications\n",
    "        \n",
    "    THIS FUNCTION IS TOO SPECIFIC TO ReDisulphID WRITTING NEW VERSION NEXT\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(in_csv, delimiter = delimiter)\n",
    "    try:\n",
    "        previous_file = pd.read_csv(out_csv, delimiter = delimiter)\n",
    "        fetched_chains = list(previous_file[\"PDB\"].drop_duplicates())\n",
    "        if debug == True:\n",
    "            print(\"already got:\", fetched_chains)\n",
    "    except:\n",
    "        previous_file = pd.DataFrame()\n",
    "        fetched_chains = []\n",
    "        if debug == True:\n",
    "            print(\"starting new PDB list\")\n",
    "    \n",
    "    apd = data[[\"PDBid\", \"a chain\"]].rename(columns = {\"a chain\" : \"chain\"})\n",
    "    bpd = data[[\"PDBid\", \"b chain\"]].rename(columns = {\"b chain\" : \"chain\"})\n",
    "    uniquechains = pd.concat([apd, bpd], ignore_index = True).reset_index(drop = True).drop_duplicates()\n",
    "\n",
    "    uniquePDBs = uniquechains[\"PDBid\"].drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    uniprotpd = pd.DataFrame({\"PDB\" : [], \"chain\": [], \"uniprot\": []})\n",
    "    i = 0\n",
    "    e = 0\n",
    "    \n",
    "    pbar = tqdm(np.setdiff1d(list(uniquePDBs), fetched_chains))\n",
    "    for PDBcode in pbar:\n",
    "        pbar.set_description(\"Getting uniprot accessions: \" + PDBcode)\n",
    "        if i < rows:\n",
    "            \n",
    "            chaindetails = {}\n",
    "            tries = 0\n",
    "            while tries < 3 and chaindetails == {}: #if no details are returned, try a couple more times\n",
    "                chaindetails = get_uniprot_accessions(PDBcode) #this is because sometimes no details are returned when there should be some, probably due to anti-scraping measures\n",
    "                tries = tries + 1\n",
    "            \n",
    "            if debug == True:\n",
    "                if tries == 3:\n",
    "                    print(\"no accessions found\")\n",
    "                if 1 < tries < 3:\n",
    "                    print(\"found accessions in\", str(tries), \"tries.\")\n",
    "            if debug == True:\n",
    "                print(chaindetails)\n",
    "            \n",
    "            for letter in chaindetails: #   Iterate each letter in the dictionary\n",
    "                #   Create a new row with the ID, chain letter, and uniprot code\n",
    "                uniprotpd.loc[e] = [PDBcode, letter, chaindetails[letter]] \n",
    "                e = e + 1\n",
    "        i = i + 1\n",
    "    \n",
    "    uniprotpd = pd.concat([uniprotpd, previous_file])\n",
    "    try:\n",
    "        uniprotpd.to_csv(out_csv, sep=delimiter, index = False)\n",
    "    except:\n",
    "        if debug == True:\n",
    "            print(\"Failed save, using utf-8 instead.\")\n",
    "        uniprotpd.to_csv(out_csv, sep=delimiter, encoding='utf-8', index = False)\n",
    "\n",
    "def iterate_uniprot_accessions(in_csv : str, chain_cols, out_csv : str, delimiter = \"\\t\", debug = True):\n",
    "    \"\"\"\n",
    "    Takes an input CSV with a PDBid header and specified custom header(s) to get the\n",
    "    uniprot ID of the chains and saves a CSV with the assession codes and unique structures\n",
    "    and chains.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_csv : str\n",
    "        The the input CSV with PDBids that is used to generate accession codes.\n",
    "    chain_cols : list or str\n",
    "        Column or list of columns that should be used to identify the chain(s) when\n",
    "        fetching their accessions.\n",
    "        e.g. [\"a chain\", \"b chain\"]\n",
    "    out_csv : str\n",
    "        The output CSV used to store fetched accession codes.\n",
    "    delimiter : str, optional\n",
    "        Delimiter of the CSV file\n",
    "    debug : bool, optional\n",
    "        Whether to print progess or other notifications\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(in_csv, delimiter = delimiter)\n",
    "    try:\n",
    "        previous_file = pd.read_csv(out_csv, delimiter = delimiter)\n",
    "        fetched_chains = list(previous_file[\"PDB\"].drop_duplicates())\n",
    "        if debug == True:\n",
    "            print(\"already got:\", fetched_chains)\n",
    "    except:\n",
    "        previous_file = pd.DataFrame()\n",
    "        fetched_chains = []\n",
    "        if debug == True:\n",
    "            print(\"starting new PDB list\")\n",
    "            \n",
    "    #   Make separate dataframes for each of the chain cols and then combine them\n",
    "    chain_dfs = []\n",
    "    uniquechains = pd.DataFrame()\n",
    "    \n",
    "    #   Parse chain_cols into a list if it is a string\n",
    "    if isinstance(chain_cols, str):\n",
    "        chain_cols = [chain_cols]\n",
    "    \n",
    "    for chain_col in chain_cols:\n",
    "        #   Make the df\n",
    "        new_df = data[[\"PDBid\", chain_col]].rename(columns = {chain_col : \"chain\"})\n",
    "        #   Combine it with the previously made df\n",
    "        uniquechains = pd.concat([uniquechains, new_df], ignore_index = True).reset_index(drop = True).drop_duplicates()\n",
    "\n",
    "    uniquePDBs = uniquechains[\"PDBid\"].drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    uniprotpd = pd.DataFrame({\"PDB\" : [], \"chain\": [], \"uniprot\": []})\n",
    "    i = 0\n",
    "    e = 0\n",
    "    \n",
    "    pbar = tqdm(np.setdiff1d(list(uniquePDBs), fetched_chains))\n",
    "    for PDBcode in pbar:\n",
    "        pbar.set_description(\"Getting uniprot accessions: \" + PDBcode)\n",
    "\n",
    "        chaindetails = {}\n",
    "        tries = 0\n",
    "        while tries < 3 and chaindetails == {}: #if no details are returned, try a couple more times\n",
    "            chaindetails = get_uniprot_accessions(PDBcode) #this is because sometimes no details are returned when there should be some, probably due to anti-scraping measures\n",
    "            tries = tries + 1\n",
    "        \n",
    "        if debug == True:\n",
    "            if tries == 3:\n",
    "                print(\"no accessions found\")\n",
    "            if 1 < tries < 3:\n",
    "                print(\"found accessions in\", str(tries), \"tries.\")\n",
    "        if debug == True:\n",
    "            print(chaindetails)\n",
    "        \n",
    "        for letter in chaindetails: #   Iterate each letter in the dictionary\n",
    "            #   Create a new row with the ID, chain letter, and uniprot code\n",
    "            uniprotpd.loc[e] = [PDBcode, letter, chaindetails[letter]] \n",
    "            e = e + 1\n",
    "\n",
    "    \n",
    "    uniprotpd = pd.concat([uniprotpd, previous_file])\n",
    "    try:\n",
    "        uniprotpd.to_csv(out_csv, sep=delimiter, index = False)\n",
    "    except:\n",
    "        if debug == True:\n",
    "            print(\"Failed save, using utf-8 instead.\")\n",
    "        uniprotpd.to_csv(out_csv, sep=delimiter, encoding='utf-8', index = False)\n",
    "        \n",
    "def get_uniprot_details(unicode : str, debug = False) -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    "    When given a uniprot code, will return a dictionary containing details from \n",
    "    the uniprot website.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unicode : str\n",
    "        The uniprot code used to fetch uniprot details.\n",
    "    debug : bool, optional\n",
    "        Whether to print notifications.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary relating to different information from uniprot, such as protein names\n",
    "        and descriptions.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        url = \"https://www.uniprot.org/uniprot/\" + unicode + \".xml\"\n",
    "        if debug == True:\n",
    "            print(\"downloading\", url)\n",
    "    except:\n",
    "        raise Exception(\"Give a uniprot accession code, recieved \" + repr(unicode))\n",
    "\n",
    "    output = {}\n",
    "    if debug == True:\n",
    "        print(f\"Extracting Uniprot information from code: {unicode}.\")\n",
    "  \n",
    "    #   Fetch the xml version of the uniprot site in beautifulsoup\n",
    "    soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
    "    \n",
    "    #   Get the protein name.\n",
    "    try:\n",
    "        output[\"uniprot name\"] = soup.find_all(\"fullname\")[0].text\n",
    "    except:\n",
    "        output[\"uniprot name\"] = np.NaN\n",
    "    \n",
    "    #   Get the uniprot abbreviation.\n",
    "    try:\n",
    "        output[\"uniprot abbreviation\"] = soup.find_all(\"name\")[0].text\n",
    "    except:\n",
    "        output[\"uniprot abbreviation\"] = np.NaN\n",
    "    \n",
    "    try:\n",
    "        #   Search for the list of localisations and then put each one in a list\n",
    "        localisations = [x.text for x in soup.find(\"comment\", {\"type\" : \"subcellular location\"}).find_all(\"location\")]\n",
    "        #   Output this list\n",
    "        output[\"localisations\"] = localisations\n",
    "    except:\n",
    "        #   If there is no such localisation\n",
    "        output[\"localisations\"] = np.NaN\n",
    "    \n",
    "    #   Get the first description\n",
    "    try:\n",
    "        output[\"description\"] = soup.find(\"title\").text\n",
    "    except:\n",
    "        #   If no description found.\n",
    "        output[\"description\"] = np.NaN\n",
    "        \n",
    "    #   Get disease variants\n",
    "    variants = {}\n",
    "    for variant in soup.find_all(\"feature\", {\"type\":\"sequence variant\"}):\n",
    "        try:\n",
    "            variant[\"description\"] # Check the variant has a description\n",
    "        except:\n",
    "            variant[\"description\"] = \"none\"\n",
    "        try: #  Check whether deletions\n",
    "            variant.find(\"original\").text\n",
    "            variant.find(\"variation\").text\n",
    "        except: #   If they are deletions (shown by no residue change)\n",
    "            try:\n",
    "                variant.append(soup.new_tag(\"original\", text = variant.find(\"position\")[\"position\"])) # Make the original residue that res nubmer\n",
    "            except: #   If its muliple residues then make that original residue a resnumber range\n",
    "                variant.append(soup.new_tag(\"original\", text = variant.find(\"begin\")[\"position\"] + \"-\" + variant.find(\"end\")[\"position\"]))\n",
    "            variant.append(soup.new_tag(\"variation\", text = \"deletion\")) #make the new residue a deletion\n",
    "        try:\n",
    "            #   Single point change\n",
    "            variants[len(variants), variant[\"description\"]] = {\"original residue\" : variant.find(\"original\").text, \"variation residue\" : variant.find(\"variation\").text, \"position\" : variant.find(\"location\").find(\"position\")[\"position\"]}\n",
    "        except:\n",
    "            #   Multi point changes\n",
    "            variants[len(variants), variant[\"description\"]] = {\"original residue\" : variant.find(\"original\").text, \"variation residue\" : variant.find(\"variation\").text, \"begin\" : variant.find(\"location\").find(\"begin\")[\"position\"], \"end\" : variant.find(\"location\").find(\"end\")[\"position\"]}\n",
    "    output[\"variants\"] = variants\n",
    "    \n",
    "    #   Get functional mutations (features)\n",
    "    mutations = {}\n",
    "    for mutation in soup.find_all(\"feature\", {\"type\":\"mutagenesis site\"}):\n",
    "        try: #  Check whether deletions\n",
    "            mutation.find(\"original\").text\n",
    "            mutation.find(\"variation\").text\n",
    "        except:\n",
    "            try: #  If they are deletions (shown by no residue change)\n",
    "                mutation.append(soup.new_tag(\"original\", text = mutation.find(\"position\")[\"position\"]))\n",
    "            except: #   If its muliple residues then make that original residue a resnumber range\n",
    "                mutation.append(soup.new_tag(\"original\", text = mutation.find(\"begin\")[\"position\"] + \"-\" + mutation.find(\"end\")[\"position\"]))\n",
    "            mutation.append(soup.new_tag(\"variation\", text = \"deletion\")) #make the new residue a deletion\n",
    "        try:\n",
    "            #   Single point mutations\n",
    "            mutations[len(mutations), mutation[\"description\"]] = {\"original residue\" : mutation.find(\"original\").text, \"variation residue\" : mutation.find(\"variation\").text, \"position\" : mutation.find(\"location\").find(\"position\")[\"position\"]}\n",
    "        except:\n",
    "            #   Sequence variants\n",
    "            mutations[len(mutations), mutation[\"description\"]] = {\"begin\" : mutation.find(\"location\").find(\"begin\")[\"position\"], \"end\" : mutation.find(\"location\").find(\"end\")[\"position\"]}\n",
    "    output[\"functional mutations\"] = mutations\n",
    "    \n",
    "    #   Get modified residues\n",
    "    modifications = {}\n",
    "    for modification in soup.find_all(\"feature\", {\"type\" : \"modified residue\"}):\n",
    "        modifications[len(modifications), modification[\"description\"]] = {\"position\" : modification.find(\"location\").find(\"position\")[\"position\"]}\n",
    "    output[\"modifications\"] = modifications\n",
    "            \n",
    "    #   Get functional sites/region of interest\n",
    "    regions = {}\n",
    "    for region in soup.find_all(\"feature\", {\"type\":[\"region of interest\", \"short sequence motif\"]}):\n",
    "        try:\n",
    "            #   Single residue\n",
    "            regions[len(regions),region[\"description\"]] = {\"position\" : region.find(\"location\").find(\"position\")[\"position\"]}\n",
    "        except:\n",
    "            #   Multiple residue\n",
    "            for location in [\"begin\", \"end\"]:\n",
    "                try: #  Test if the begining and end have positions\n",
    "                    test = region.find(location)[\"position\"]\n",
    "                except: #   Otherwise assign a new position from the status (likely \"unknown\")\n",
    "                    region.find(location)[\"position\"] = region.find(location)[\"status\"]\n",
    "            regions[len(regions),region[\"description\"]] = {\"begin\" : region.find(\"location\").find(\"begin\")[\"position\"], \"end\" : region.find(\"location\").find(\"end\")[\"position\"]}\n",
    "    output[\"regions\"] = regions\n",
    "        \n",
    "    #   Get family & domains\n",
    "    domains = {}\n",
    "    for domain in soup.find_all(\"feature\", {\"type\":\"domain\"}):\n",
    "        try:\n",
    "            #   Single residue domain\n",
    "            domains[len(domains), domain[\"description\"]] = {\"position\" : domain.find(\"location\").find(\"position\")[\"position\"]}\n",
    "        except:\n",
    "            #   Multi residue domain\n",
    "            domains[len(domains), domain[\"description\"]] = {\"begin\" : domain.find(\"location\").find(\"begin\")[\"position\"], \"end\" : domain.find(\"location\").find(\"end\")[\"position\"]}\n",
    "    output[\"domains\"] = domains\n",
    "    \n",
    "    #   Active site\n",
    "    active_sites = {}\n",
    "    for active_site in soup.find_all(\"feature\", {\"type\":[\"active site\"]}):\n",
    "        try:\n",
    "            active_site[\"description\"] #    Test whether the active site has a description\n",
    "        except:\n",
    "            active_site[\"description\"] = \"none\"\n",
    "\n",
    "        active_sites[len(active_sites),active_site[\"description\"]] = {\"position\" : active_site.find(\"location\").find(\"position\")[\"position\"]}\n",
    "            \n",
    "    output[\"active sites\"] = active_sites\n",
    "    \n",
    "    #   Binding site\n",
    "    binding_sites = {}\n",
    "    for binding_site in soup.find_all(\"feature\", {\"type\" : [\"binding site\"]}):\n",
    "        try:\n",
    "            #   Single binding residue\n",
    "            binding_sites[len(binding_sites),binding_site.find(\"ligand\").find(\"name\").text] = {\"position\" : binding_site.find(\"location\").find(\"position\")[\"position\"], \"reference\" : binding_site.find(\"ligand\").find(\"dbreference\")[\"id\"]}\n",
    "            try:\n",
    "                binding_sites[len(binding_sites),binding_site.find(\"ligand\").find(\"name\").text] = []\n",
    "                binding_sites[len(binding_sites),binding_site.find(\"ligand\").find(\"name\").text] = {\"position\" : binding_site.find(\"location\").find(\"position\")[\"position\"]} #if there is no DB reference\n",
    "            except:\n",
    "                #   Binding sequence\n",
    "                binding_sites[len(binding_sites),binding_site.find(\"ligand\").find(\"name\").text] = {\"begin\" : binding_site.find(\"location\").find(\"begin\")[\"position\"], \"end\" : binding_site.find(\"location\").find(\"end\")[\"position\"]}\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    output[\"binding sites\"] = binding_sites\n",
    "    \n",
    "    \n",
    "    #   Default mammalian to 0\n",
    "    output[\"mammalian\"] = 0\n",
    "    #   Find all taxons and convert them to a list, then check if one of them is mammalian\n",
    "    if \"Mammalia\"  in [x.text for x in soup.find_all(\"taxon\")]:\n",
    "        output[\"mammalian\"] = 1\n",
    "    \n",
    "    #   Get all the terms where processes are mixed up with component and molecular function\n",
    "    processes = [x.get(\"value\") for x in soup.find_all(\"property\", {\"type\" : \"term\"})]\n",
    "    #   Strip out only the processes\n",
    "    output[\"function\"] = [x.split(\":\")[1] for x in processes if \"P:\" in x]\n",
    "     \n",
    "    if debug == True:\n",
    "        for i in output:\n",
    "            print(i, output[i])\n",
    "    return output\n",
    "\n",
    "#OLD TOO SPECIFIC CODE:\n",
    "def iterate_uniprot_details_OLD(in_csv : str, out_csv : str, uniprot_csv : str = None, species : str = None, debug = True):\n",
    "    \"\"\"\n",
    "    Takes an input CSV, uses chain uniprot accessions to add uniprot data to a screen\n",
    "    product, which is then output as another CSV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_csv : str\n",
    "        The filepath to an input CSV, which must contain columns \"uniprot a\" and \n",
    "        \"uniprot b\".\n",
    "    out_csv : str\n",
    "        The intended filepath of the output CSV.\n",
    "    uniprot_csv : str, optional\n",
    "        If a uniprot CSV is then specified use that. The default is None, in which \n",
    "        case a column in the input CSV can be used. NEEDS CONFIRMATION\n",
    "    species : str, optional\n",
    "        Can specify a species if there is a species-specific column of uniprots \n",
    "        that should be used for fetching data. The default is None.\n",
    "        If a species is specified then it will look in that column for the uniprot \n",
    "        accession, rather than fetching the uniprot accession from the separate CSV.\n",
    "    debug : str, optional\n",
    "        Whether to print progess.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A CSV file of the input database populated with protein information from uniprot.\n",
    "    \n",
    "    New columns will be the species, then \"a chain\", or \"b chain\" followed by:\n",
    "        'uniprot name', 'uniprot abbreviation', 'localisations', 'description', 'variants',\n",
    "        'functional mutations', 'modifications', 'regions', 'domains', 'active sites', \n",
    "        'binding sites', 'mammalian', and 'function'.\n",
    "        \n",
    "    THIS FUNCTION IS TOO SPECIFIC TO ReDisulphID\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #   Converts the screen product and uniprot csvs to pandas dataframes\n",
    "    data = pd.read_csv(in_csv, delimiter = \"\\t\")\n",
    "    if uniprot_csv != None:\n",
    "        uniprotdata = pd.read_csv(uniprot_csv, delimiter = \"\\t\")\n",
    "    \n",
    "    chains = [\"a\", \"b\"]\n",
    "    #   Combines the known uniprot accessions into the screen product dataframe\n",
    "    for chain in chains: #  Adds the uniprot based on on 'a chain' then 'b chain'\n",
    "        if uniprot_csv != None: #   If uniprotdata was specified, then add this to the dataframe\n",
    "            data = pd.merge(data, uniprotdata, how = \"left\", left_on=[\"PDBid\", chain + \" chain\"], right_on=[\"PDB\", \"chain\"]) #  Looks at the chain letter and pdb accession code and adds the row from the uniprot dataframe\n",
    "            data = data.drop([\"PDB\", \"chain\"], axis = 1) #  Removes the added PDB code and chain since this information is duplicate\n",
    "            if species == None:\n",
    "                #   Renames the new 'chain' column to specify a or b\n",
    "                data = data.rename(columns = {\"uniprot\" : \"uniprot \" + chain}) #    \n",
    "            else:\n",
    "                data = data.rename(columns = {\"uniprot\" : chain + \" \" + species + \" accession code\"})\n",
    "    \n",
    "    data = data.loc[:,~data.columns.duplicated()].copy()\n",
    "    \n",
    "    #if a species is specified, then look in that column for the accession code otherwise look in default column\n",
    "    accession_columns = [\"a\", \"b\"]\n",
    "    if species == None:\n",
    "        accession_columns = [\"uniprot \" + x for x in chains] #default column\n",
    "    else:\n",
    "        try:\n",
    "            accession_columns = [x + \" \" + species + \" accession code\" for x in chains]\n",
    "        except:\n",
    "            raise Exception(\"Expected species as string, got \" + repr(species))\n",
    "    \n",
    "    #accesses the internet using the new uniprots to get uniprot data using get_uniprot_details()\n",
    "    #creates a new series of unique uniprot codes\n",
    "\n",
    "    uniqueuniprots = pd.concat([data[\"uniprot a\"], data[\"uniprot b\"]],\n",
    "                               ignore_index = True).reset_index(drop = True).drop_duplicates()\n",
    "    uniqueuniprots = pd.DataFrame(uniqueuniprots) #turns this series into a 1 column dataframe\n",
    "    uniqueuniprots.columns = ([\"uniprot\"]) #gives the 1 column the name 'uniprots'\n",
    "    uniqueuniprots = uniqueuniprots.dropna() #drops any empty enties\n",
    "    #drops entries that say \"no accession code\" (which were returned before when they couldn't be found)\n",
    "    uniqueuniprots = uniqueuniprots[uniqueuniprots[\"uniprot\"] != \"no accession code\"]\n",
    "    \n",
    "    #creates a test data row based on XRCC4 so we know what the columns are called   \n",
    "    testset = get_uniprot_details(\"Q13426\")\n",
    "    testset[\"uniprot\"] = \"Q13426\"\n",
    "    dataset = pd.DataFrame(columns = testset.keys()) #creates a new empty dataframe with those columns\n",
    "    \n",
    "    #iterates through the uniprots and gets the extra information from the internet added to the new dataset dataframe\n",
    "    dex = 0\n",
    "    for index, row in tqdm(uniqueuniprots.iterrows(), total = uniqueuniprots.shape[0]): #iterates through the unique uniprots\n",
    "        #print(dex, \"/\", uniqueuniprots.size) #indicates the progress\n",
    "        dex = dex + 1\n",
    "        \n",
    "        if len(row[\"uniprot\"]) > 4: # Check the uniprot code is valid\n",
    "            # Get uniprot information as a dictionary\n",
    "            newinfo = get_uniprot_details(row[\"uniprot\"])\n",
    "            # Add the uniprot accession code to that dictionary\n",
    "            newinfo[\"uniprot\"] = row[\"uniprot\"]\n",
    "            # Create a 1 row dataframe of that dictionary, with the keys as the columns \n",
    "            for i in newinfo:\n",
    "                print(f\"{i}: {newinfo[i]}\")\n",
    "            newinfo = pd.DataFrame([newinfo],  columns = newinfo.keys())\n",
    "            # Combine that with the previously constructed dataframe\n",
    "            dataset = pd.concat([dataset, newinfo], ignore_index = True) # adds 1 row to the dataset dataframe\n",
    "        else:\n",
    "            if debug == True:\n",
    "                print(\"not a uniprot accession:\", row[\"uniprot\"])\n",
    "            \n",
    "            \n",
    "    #convert species column into dictionary for better access\n",
    "    accession_columns = {\"a chain\": accession_columns[0], \"b chain\" : accession_columns[1]}\n",
    "    if species == None: #convert the species into text that can be used to name columns\n",
    "        species = \"\"\n",
    "    else:\n",
    "        species = species + \" \"\n",
    "    \n",
    "    #from the uniprots given by each chain, adds the information from the unique uniprot list to the screen data\n",
    "\n",
    "    for chain in [\"a chain\", \"b chain\"]: #iterate between the two chains\n",
    "        data = pd.merge(data, dataset, how=\"left\", left_on=[accession_columns[chain]], right_on=[\"uniprot\"]).reset_index(drop = True) #merges the uniprot data to the screen based on the chain uniprots\n",
    "        data = data.drop([\"uniprot\"], axis = 1) #drop the new uniprot column since it is a duplicate\n",
    "        data = data.rename(columns = dict(zip(newinfo.keys(), species + chain + \" \" + newinfo.keys()))) #renames the new columns to specify their chains\n",
    "    \n",
    "    #save all this as a new CSV \n",
    "    data.to_csv(out_csv, sep=\"\\t\", index = False)\n",
    "    \n",
    "def iterate_uniprot_details(in_csv : str,\n",
    "                            chain_cols : list,\n",
    "                            out_csv : str,\n",
    "                            delimiter : str = \"\\t\",\n",
    "                            uniprot_csv : str = None,\n",
    "                            species : str = None,\n",
    "                            debug = True):\n",
    "    \"\"\"\n",
    "    Takes an input pandas DataFrame or CSV file, uses uniprot accessions and provided chain column(s)\n",
    "    to add uniprot information columns to the data, which is then output as another CSV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_csv : str\n",
    "        The filepath to an input CSV, which must contain columns \"uniprot a\" and \n",
    "        \"uniprot b\".\n",
    "    chain_cols : list or str\n",
    "        String or list of strings specifying which column to get the identity of the \n",
    "        peptide chain from.\n",
    "        E.g. [\"a chain\", \"b chain\"]\n",
    "    out_csv : str\n",
    "        The intended filepath of the output CSV.\n",
    "    delimiter : str, optional\n",
    "        Delimiter that is used in the input CSV, and should be used in the output CSV.\n",
    "    uniprot_csv : str, optional\n",
    "        If a uniprot CSV is then specified use that. The default is None, in which \n",
    "        case a column in the input CSV can be used. NEEDS CONFIRMATION\n",
    "    species : str, optional\n",
    "        Can specify a species if there is a species-specific column of uniprots \n",
    "        that should be used for fetching data. The default is None.\n",
    "        If a species is specified then it will look in that column for the uniprot \n",
    "        accession, rather than fetching the uniprot accession from the separate CSV.\n",
    "    debug : str, optional\n",
    "        Whether to print progess.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A CSV file of the input database populated with protein information from uniprot.\n",
    "    \n",
    "    New columns will be the species, then specified chain columns followed by:\n",
    "        'uniprot name', 'uniprot abbreviation', 'localisations', 'description', 'variants',\n",
    "        'functional mutations', 'modifications', 'regions', 'domains', 'active sites', \n",
    "        'binding sites', 'mammalian', and 'function'.\n",
    "        \n",
    "    EDITING TO BE NON SPECIFIC\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #   Converts the screen product and uniprot csvs to pandas dataframes\n",
    "    if isinstance(in_csv, str):\n",
    "        data = pd.read_csv(in_csv, delimiter = delimiter)\n",
    "    if isinstance(uniprot_csv, str):\n",
    "        uniprot_csv = pd.read_csv(uniprot_csv, delimiter = delimiter)\n",
    "        \n",
    "    #   Parse chain_cols\n",
    "    if isinstance(chain_cols, str):\n",
    "        chain_cols = [chain_cols]\n",
    "    \n",
    "    #   Combines the known uniprot accessions into the screen product dataframe\n",
    "    for chain_col in chain_cols: #  Adds the uniprot based on on 'a chain' then 'b chain'\n",
    "        if uniprot_csv != None: #   If uniprot_csv was specified, then add this to the dataframe\n",
    "            data = pd.merge(data, uniprot_csv, how = \"left\", left_on=[\"PDBid\", chain_col], right_on=[\"PDB\", \"chain\"]) #  Looks at the chain letter and pdb accession code and adds the row from the uniprot dataframe\n",
    "            data = data.drop([\"PDB\", \"chain\"], axis = 1) #  Removes the added PDB code and chain since this information is duplicate\n",
    "            if species == None:\n",
    "                #Renames the new 'chain' column to specify the chain\n",
    "                data = data.rename(columns = {\"uniprot\" : \"uniprot \" + chain_col}) #    \n",
    "            else:\n",
    "                data = data.rename(columns = {\"uniprot\" : chain_col + \" \" + species + \" accession code\"})\n",
    "    \n",
    "    data = data.loc[:,~data.columns.duplicated()].copy()\n",
    "    \n",
    "    # If a species is specified, then look in that column for the accession\n",
    "    # code, otherwise look in default column defined by the species.\n",
    "    accession_columns = [\"a\", \"b\"]\n",
    "    if species is None:\n",
    "        accession_columns = [\"uniprot \" + x for x in accession_columns] #default column\n",
    "    else:\n",
    "        assert isinstance(species, str), \"Expected species as string, found \" + repr(species)\n",
    "        accession_columns = [x + \" \" + species + \" accession code\" for x in accession_columns]\n",
    "    \n",
    "    #accesses the internet using the new uniprots to get uniprot data using get_uniprot_details()\n",
    "    #creates a new series of unique uniprot codes\n",
    "            \n",
    "    uniqueuniprots = pd.concat([data[\"uniprot a\"], data[\"uniprot b\"]],\n",
    "                               ignore_index = True).reset_index(drop = True).drop_duplicates()\n",
    "    uniqueuniprots = pd.DataFrame(uniqueuniprots) #turns this series into a 1 column dataframe\n",
    "    uniqueuniprots.columns = ([\"uniprot\"]) #gives the 1 column the name 'uniprots'\n",
    "    uniqueuniprots = uniqueuniprots.dropna() #drops any empty enties\n",
    "    #drops entries that say \"no accession code\" (which were returned before when they couldn't be found)\n",
    "    uniqueuniprots = uniqueuniprots[uniqueuniprots[\"uniprot\"] != \"no accession code\"]\n",
    "    \n",
    "    #creates a test data row based on XRCC4 so we know what the columns are called   \n",
    "    testset = get_uniprot_details(\"Q13426\")\n",
    "    testset[\"uniprot\"] = \"Q13426\"\n",
    "    dataset = pd.DataFrame(columns = testset.keys()) #creates a new empty dataframe with those columns\n",
    "    \n",
    "    #iterates through the uniprots and gets the extra information from the internet added to the new dataset dataframe\n",
    "    dex = 0\n",
    "    for index, row in tqdm(uniqueuniprots.iterrows(), total = uniqueuniprots.shape[0]): #iterates through the unique uniprots\n",
    "        #print(dex, \"/\", uniqueuniprots.size) #indicates the progress\n",
    "        dex = dex + 1\n",
    "        \n",
    "        if len(row[\"uniprot\"]) > 4:\n",
    "            newinfo = get_uniprot_details(row[\"uniprot\"]) #gets uniprot information as a dictionary\n",
    "            newinfo[\"uniprot\"] = row[\"uniprot\"] #adds the uniprot accession code to that dictionary\n",
    "            #creates a 1 row dataframe of that dictionary, with the keys as the columns \n",
    "            newinfo = pd.DataFrame([newinfo],  columns = newinfo.keys())\n",
    "            dataset = pd.concat([dataset, newinfo], ignore_index = True) # adds 1 row to the dataset dataframe\n",
    "        else:\n",
    "            if debug == True:\n",
    "                print(\"not a uniprot accession:\", row[\"uniprot\"])\n",
    "            \n",
    "            \n",
    "    #convert species column into dictionary for better access\n",
    "    accession_columns = {\"a chain\": accession_columns[0], \"b chain\" : accession_columns[1]}\n",
    "    if species == None: #convert the species into text that can be used to name columns\n",
    "        species = \"\"\n",
    "    else:\n",
    "        species = species + \" \"\n",
    "    \n",
    "    #from the uniprots given by each chain, adds the information from the unique uniprot list to the screen data\n",
    "\n",
    "    for chain in [\"a chain\", \"b chain\"]: #iterate between the two chains\n",
    "        data = pd.merge(data, dataset, how=\"left\", left_on=[accession_columns[chain]], right_on=[\"uniprot\"]).reset_index(drop = True) #merges the uniprot data to the screen based on the chain uniprots\n",
    "        data = data.drop([\"uniprot\"], axis = 1) #drop the new uniprot column since it is a duplicate\n",
    "        data = data.rename(columns = dict(zip(newinfo.keys(), species + chain + \" \" + newinfo.keys()))) #renames the new columns to specify their chains\n",
    "    \n",
    "    #save all this as a new CSV \n",
    "    data.to_csv(out_csv, sep=\"\\t\", index = False)\n",
    "\n",
    "parser = PDBParser()\n",
    "def get_flanking_info(PDB_file : str, amino_acid : str, debug : bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "        \n",
    "    Takes a PDB file and returns flanking information for all the cysteines and also returns the real sequences of each chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    PDB_file : str\n",
    "        The path to the input PDB file.\n",
    "    amino_acid : str\n",
    "        The amino acid to list flanking info for, e.g. \"CYS\".\n",
    "    debug : bool, optional\n",
    "        Should this function print its output each time.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        [0]: Dataframe with information about flanking residues including sequence, pKas, hydrophobicities, \n",
    "        and charges at pH 7.\n",
    "        \n",
    "        [1]: Sequences of the chains in the structure.\n",
    "        \n",
    "    \"\"\"\n",
    "    #catch input errors\n",
    "    assert isinstance(PDB_file, str), \"str expected for PDB_file, found '\" + repr(PDB_file) + \"' which is \" + repr(type(PDB_file))\n",
    "    assert isinstance(debug, bool), \"bool expected for debug, found \" + repr(type(debug))\n",
    "    \n",
    "    cysteine_list = []\n",
    "    with gzip.open(PDB_file, \"rt\") as unzipped: #open the structure\n",
    "        structure = parser.get_structure(\"struc\", unzipped) #parse the structure\n",
    "        chain_sequences = {}\n",
    "        for chain in structure[0]: #iterate through chains\n",
    "            realreslist = [] #make a list to store the residues in a chain\n",
    "            length = max([i.id[1] for i in chain]) #determine how long the chain actually is, ignoring gaps\n",
    "            res_list = [\"!\" for i in range(length)] #creating reslist by starting with blank ! marks\n",
    "            for index, residue in enumerate(chain):\n",
    "                #populating realreslist\n",
    "                residue.newresnum = index\n",
    "                realreslist.append(residue)\n",
    "                \n",
    "                #populating res_list, which has gaps\n",
    "                resname = residue.get_resname()\n",
    "                try: \n",
    "                    if resname != \"HOH\" and residue.id[1] >= 0: #check residue is not water and has a seq number of 0 or more\n",
    "                        res_list[residue.id[1] -1] = threetoone[resname] #compile chain sequence\n",
    "                except:\n",
    "                    if debug == True: print(\"res list:\", res_list, \"residue:\", residue)\n",
    "                    res_list[residue.id[1] -1] = \"!\" #otherwise add exclamation marks\n",
    "            for residue in realreslist:\n",
    "                if residue.get_resname() == amino_acid:\n",
    "                    flanks = []\n",
    "                    for offset in range(-5, 6):\n",
    "                        try:\n",
    "                            flanks.append(threetoone[realreslist[residue.newresnum + offset].get_resname()])\n",
    "                        except:\n",
    "                            flanks.append(\"!\")\n",
    "                    pKas = []\n",
    "                    hyd = []\n",
    "                    charge = []\n",
    "                    for flank in flanks:\n",
    "                        try:\n",
    "                            pKas.append(get_pKa[flank])\n",
    "                        except:\n",
    "                            pKas.append(\"\")\n",
    "                        try:\n",
    "                            hyd.append(get_hydrophobic7[flank])\n",
    "                        except:\n",
    "                            hyd.append(\"\")\n",
    "                        try:\n",
    "                            charge.append(get_charge[flank])\n",
    "                        except:\n",
    "                            charge.append(\"\")\n",
    "                    cysteine_list.append({\"PDBid\" : PDB_file.split(\"pdb\")[-1].split(\".ent\")[0],\n",
    "                                          \"chain\" : chain.id,\n",
    "                                          \"residue\" : residue.id[1],\n",
    "                                          \"flanking residues\" : flanks,\n",
    "                                          \"flanking pKas\" : pKas,\n",
    "                                          \"flanking hydrophobicities\": hyd,\n",
    "                                          \"flanking charges at pH 7\": charge})\n",
    "                    \n",
    "            chain_sequences[chain.id] = \"\".join(res_list) #join the res_list compiled previously into strings add to a dictionary with the chain letters as keys\n",
    "    \n",
    "    newdata = pd.DataFrame() #new empty dataframe to start building\n",
    "    for cysteine in cysteine_list:\n",
    "        pdrow = pd.DataFrame([[cysteine[\"PDBid\"], cysteine[\"chain\"], cysteine[\"residue\"]]], columns = [\"PDBid\",\"chain\", \"residue\"])\n",
    "        addlettersto = []\n",
    "        for section in [\"flanking residues\", \"flanking pKas\", \"flanking hydrophobicities\", \"flanking charges at pH 7\"]: #iterates through the lists in the dictionary created by get_flanking_info()\n",
    "                for i in range(len(cysteine[section])): #iterates through each list\n",
    "                    #print(\"i:\", i, \"- section:\", section, \"- newrow:\", newrow)\n",
    "                    pdrow[str(i - 5) + \" \" + section] = cysteine[section][i] #adds each bit of information on the list as its own column in the empty dataframe, so there is one row\n",
    "                    addlettersto.append(str(i - 5) + \" \" + section)\n",
    "        newdata = pd.concat([newdata, pdrow], ignore_index = True).reset_index(drop = True) #concats this new row into a dataframe to build it\n",
    "    #print(chain_sequences)\n",
    "    return newdata, chain_sequences #output the flanking info as a new dataframe for each cysteine, output the chain sequences as a dicitonary\n",
    "\n",
    "def get_residues(residue : str, flanknum : int, sequence : str, placeholder : str = \"!\", frameshift : bool = False, strict = False) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a sequence. Creates a dictionary with the residue number and flanking residues \n",
    "    of an amino acid that can be found in the sequence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    residue : str\n",
    "        The residue to return a list of. E.g. \"C\".\n",
    "    flanknum : int\n",
    "        Number of residues to return in the sequences wither side of the desired residue.\n",
    "    sequence : str\n",
    "        Sequence in which to find the residues.\n",
    "    placeholder : str, optional\n",
    "        Placeholder where there is empty space, such as after the end of a sequence. \n",
    "        The default is \"!\".\n",
    "    frameshift : bool, optional\n",
    "        Whether to include also returning frameshifted versions of the sequence, to \n",
    "        make sequence alignment more robust. The default is False.\n",
    "        Frameshift is a bit of a misnomer, just mean (for example) if a residue has \n",
    "        been swapped with another next to it, like ABCDEFG compared to ABCEDFG which \n",
    "        would otherwise lead to not detecting that residue at all.\n",
    "    strict : bool, optional\n",
    "        Whether to raise an exception if the input sequence isn't valid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing sequence numbers as keys with the data attached to each \n",
    "        key being a sequence around that residue.\n",
    "        If frameshift is True, some entries will be frameshifted sequences with a key\n",
    "        as the residue number followed by +/- and a number denoting the number of \n",
    "        frameshifts.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if strict == True:\n",
    "        assert isinstance(sequence, str), \"Expected str for sequence, got '\" + repr(sequence) + \"' which is \" + repr(type(sequence))\n",
    "\n",
    "    # The residue number is the actual number rather the list-index number\n",
    "    reslist = {} # Make the dictionary to fill up\n",
    "    sequence = \"\".join([placeholder for i in range(flanknum)]) + sequence\n",
    "\n",
    "    for position, letter in enumerate(str(sequence)): #iterate through the letters in the sequence\n",
    "        if(letter == residue): #check that the letter is a cysteine? allowing return of any residue caused problems\n",
    "            flank = []\n",
    "            for offset in range(0 - flanknum, flanknum + 1): #count through the residues around the residue\n",
    "                try:\n",
    "                    flank.append(sequence[position + offset]) #try to add those flanking residues to 'flank'\n",
    "                except:\n",
    "                    flank.append(placeholder) #what to add when there isn't a residue\n",
    "            reslist[position - flanknum + 1] =  \"\".join(flank) #add the residue and flanks to the list\n",
    "    \n",
    "    #   Add new potential frameshifted residues        \n",
    "    if frameshift != False:\n",
    "        shifted = {}\n",
    "        for res in reslist:    #   For every seq1 residue\n",
    "        \n",
    "            #   Set the frameshift level to the flanknum if frameshift not specified\n",
    "            if isinstance(frameshift, bool):\n",
    "                frameshift = flanknum\n",
    "            \n",
    "            #   Frameshift every potential residue on the left side\n",
    "            #   For each number of frameshifts up to the flanknum\n",
    "            for shift in range(frameshift):\n",
    "                #   Make a new entry to store the shifted sequence as a list\n",
    "                shifted[str(res) + \"-\" + str(shift)] = list(reslist[res])\n",
    "                #   Add an X for every frameshift and remove a residue to compensate\n",
    "                for i in range(shift):\n",
    "                    shifted[str(res) + \"-\" + str(shift)].insert(flanknum, \"X\")\n",
    "                    shifted[str(res) + \"-\" + str(shift)].pop(0)\n",
    "                    \n",
    "            #   Frameshift every potential residue on the right side\n",
    "            for shift in range(frameshift):\n",
    "                shifted[str(res) + \"+\" + str(shift)] = list(reslist[res])\n",
    "                for i in range(shift):\n",
    "                    shifted[str(res) + \"+\" + str(shift)].insert(flanknum + 1, \"X\")\n",
    "                    shifted[str(res) + \"+\" + str(shift)].pop()\n",
    "        \n",
    "        #   Concatonate the shifted sequences into strings so they are the same as \n",
    "        #   normal ones.\n",
    "        for res in shifted:\n",
    "            shifted[res] = \"\".join(shifted[res])\n",
    "        #   Add the shifted sequences to the normal ones\n",
    "        reslist.update(shifted)\n",
    "        \n",
    "    return reslist\n",
    "\n",
    "amino_acid_groups = {False: [], \"positive\" : [\"R\", \"H\", \"K\"], \"negative\": [\"D\", \"E\"], \"polar\": [\"S\", \"T\", \"N\", \"Q\"], \"hydrophobic\":[\"A\", \"V\", \"I\", \"L\", \"M\", \"F\", \"W\"]}\n",
    "def get_equivalentresidue(resnum : int, seq1 : str, seq2 : str, flanknum : int = 5, placeholder : str = \"!\", pass_nan : bool = True, debug : bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Takes the specified residue from sequence 1 and uses alignment to get its number \n",
    "    in sequence 2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    resnum : int\n",
    "        The residue number to convert, starts at 1.\n",
    "    seq1 : str\n",
    "        The sequence that residue number is from.\n",
    "    seq2 : str\n",
    "        The sequence to find that residue in.\n",
    "    flanknum : int, optional\n",
    "        The number of flanking residues to use in the alignment. The default is 5.\n",
    "    placeholder : str, optional\n",
    "        The placeholder to be used when a residue is missing, such as the beginning \n",
    "        and ends of the sequence. Must be one character. The default is \"!\".\n",
    "    pass_nan : bool, optional\n",
    "        Whether to pass or raise an exception when given nan in the sequences\n",
    "    debug : bool, optional\n",
    "        Whether a message should be printed when failing to find a residue. The default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        [0]: converted residue number\n",
    "        [1]: alignment score\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"resnum:\", resnum, \"\\tseq1:\", seq1, \"\\tseq2:\", seq2, \"\\tplaceholder:\", placeholder)\n",
    "    #print(get_residues(seq1[resnum - 1], 5, seq1, placeholder))\n",
    "    #print(seq1, resnum)\n",
    "    \n",
    "    #catch input errors\n",
    "    assert isinstance(resnum, int), \"Expected int for resnum, got \" + repr(type(resnum))\n",
    "    assert isinstance(pass_nan, bool), \"Expected bool for pass_nan, got \" + repr(type(pass_nan))\n",
    "    assert isinstance(flanknum, int), \"Expected int for flanknum, got \" + repr(type(flanknum))\n",
    "    assert isinstance(placeholder, str), \"Expected str for placeholder, got \" + repr(type(placeholder))\n",
    "    assert len(placeholder) == 1, f\"Expected single letter string for placeholder, got '{placeholder}'.\"\n",
    "    assert isinstance(debug, bool), \"Expected bool for debug, got \" + repr(type(debug))\n",
    "    \n",
    "    if pass_nan == False:\n",
    "        assert isinstance(seq1, str), \"Expected str for seq1, got '\" + repr(seq1) + \"' which is \" + repr(type(seq1))\n",
    "        assert isinstance(seq2, str), \"Expected str for seq2, got '\" + repr(seq2) + \"' which is \" + repr(type(seq2))\n",
    "    else:\n",
    "        if isinstance(seq1, str) == False:\n",
    "            return [np.NaN, np.NaN]\n",
    "        if isinstance(seq2, str) == False:\n",
    "            return [np.NaN, np.NaN]\n",
    "\n",
    "    #   Extract the flanking sequence in seq1\n",
    "    failed = False\n",
    "    try:\n",
    "        extract = get_residues(seq1[resnum - 1], flanknum, seq1, placeholder)[resnum]\n",
    "    except:\n",
    "        failed = True\n",
    "        with open(\"converting regions errors.csv\", \"a+\") as f:\n",
    "            f.write(f\"No residue {resnum} in {seq1} to convert to {seq2}\\r\")\n",
    "    \n",
    "    if failed == False:\n",
    "        #   Turn seq2 into a dictionary of its relevent residues\n",
    "        seq2 = get_residues(seq1[resnum - 1], flanknum, seq2, placeholder, frameshift = 1)\n",
    "        \n",
    "        highscore = 0\n",
    "        highscorer = np.NaN\n",
    "        for potential in seq2:  #   For potential residues in the seq2 dictionary\n",
    "            score = 0\n",
    "            for seq2index, letter in enumerate(seq2[potential]):\n",
    "                group = False\n",
    "                for key in amino_acid_groups:\n",
    "                    if letter in amino_acid_groups[key]:\n",
    "                        group = key\n",
    "                if(letter == extract[seq2index]):\n",
    "                    score = score + 1\n",
    "                elif extract[seq2index] in amino_acid_groups[group]:\n",
    "                    score = score + 0.5\n",
    "                    \n",
    "            if score > highscore:\n",
    "                highscorer = potential\n",
    "                highscore = score\n",
    "                \n",
    "        #   highscorer will be a string if it was a frameshifted residue, so turn this\n",
    "        #   back to a plain integer. Also needs to remove the +/- frameshift.\n",
    "        if isinstance(highscorer, str):\n",
    "            highscorer = int(highscorer.split(\"-\")[0].split(\"+\")[0])\n",
    "                \n",
    "        return [highscorer, highscore]\n",
    "    else:\n",
    "        return [np.NaN, np.NaN]\n",
    "    #except:\n",
    "    #    if debug == True: print(\"Failed to get equivalent residue. Resnum:\", resnum, \" Seq1:\", seq1, \" Seq2:\", seq2)\n",
    "    #    return [np.NaN, np.NaN]\n",
    "\n",
    "def reverse_sequence(sequence, seq_start = False, seq_end = False):\n",
    "    \"\"\"\n",
    "    Takes input characters and reverses it. If seq_start and seq_end are given, function\n",
    "    returns where seq_start and seq_end are in the new sequence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    sequence : str\n",
    "        The sequence to reverse.\n",
    "    seq_start : int, optional\n",
    "        Region position that will be returned in the new sequence. The default is False.\n",
    "    seq_end : int, optional\n",
    "        Region position that will be returned in the new sequence. The default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or\n",
    "    tuple\n",
    "        [0]: The reversed sequence.\n",
    "        [1]: The new start position of the region.\n",
    "        [2]: The new end position of the region.\n",
    "    \"\"\"\n",
    "\n",
    "    # If no start or end is given, just reverse the sequence\n",
    "    if seq_start == False and seq_end == False:\n",
    "        return sequence[::-1]\n",
    "    assert isinstance(seq_start, int), \"Expected int for seq_start, got \" + repr(type(seq_start))\n",
    "    assert isinstance(seq_end, int), \"Expected int for seq_end, got \" + repr(type(seq_end))\n",
    "\n",
    "    #   If start and end are given, return the new start and end positions\n",
    "    end = len(sequence)\n",
    "    new_start = end - seq_end\n",
    "    new_end = end - seq_start\n",
    "    return sequence[::-1], new_start + 1, new_end + 1\n",
    "\n",
    "def convert_region(start_sequence: str, start_region : Union[int, list], end_sequence : str, debug = False) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a start_region in start_sequence and returns where this region is in end_sequence. \n",
    "    Use biological sequence numbers (start at 1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_sequence : str\n",
    "        The sequence which the known region belongs to.\n",
    "    start_region : int | list\n",
    "        The residue or residue range (as a list) that this region occupies.\n",
    "    end_sequence : str\n",
    "        The new sequence where this region should be detected.\n",
    "    debug : bool, optional\n",
    "        Should progress be printed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Contains \"start\" and \"end\" as the start and end of the sequence, and \"score\" \n",
    "        as the alignment score of the new region.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #   Check if start_region is nan, if so, just return NaN\n",
    "    if isinstance(start_region, list) == False:\n",
    "        if pd.isnull(start_region):\n",
    "            return {\"start\" : np.NaN, \"end\": np.NaN, \"score\" : np.NaN}\n",
    "    \n",
    "    #   Assert some stuff about the sequence\n",
    "    assert isinstance(start_sequence, str), \"Expected string for start_sequence, got \" + repr(type(start_sequence))\n",
    "    assert type(start_region) in [list, int], \"Expected list or int for start_region, got \" + repr(type(start_region))\n",
    "    assert isinstance(end_sequence, str), \"Expected string for end_sequence, got \" + repr(type(end_sequence))\n",
    "    \n",
    "    #   If only one number is presented for start region, duplicate it to give a range \n",
    "    #   of just one amino acid.\n",
    "    try:\n",
    "        #   If list of one, duplicate it.\n",
    "        if len(start_region) == 1:  \n",
    "            start_region.append(start_region[0])      \n",
    "            \n",
    "        #   If one number in the range is np.NaN, duplicate the other number into that spot.\n",
    "        if pd.isnull(start_region[0]):\n",
    "            start_region[0] = start_region[0]\n",
    "        if pd.isnull(start_region[1]):\n",
    "            start_region[1] = start_region[0]\n",
    "        #   If either number is now np.NaN, this means both were np.NaN, so just return \n",
    "        #   nan values.\n",
    "        if np.NaN in start_region:\n",
    "            return {\"start\" : np.NaN, \"end\": np.NaN, \"score\" : np.NaN}\n",
    "    except:\n",
    "        start_region = [int(start_region), (start_region)] # If not a list, duplicate into a list.\n",
    "    start_sequence = \"X\" + start_sequence #add an X to the begining of start sequence to fix indexing\n",
    "    end_sequence = \"X\" + end_sequence #add an X to the begining of end sequence to fix indexing\n",
    "    \n",
    "    #  Start saving some residues\n",
    "    residues = []\n",
    "    try:\n",
    "        seq_range = range(start_region[0], start_region[1] + 1) #correct for addition of X and make it inclusive\n",
    "    except:\n",
    "        raise Exception(\"\"\"start_region must be number or list of two positive numbers to \n",
    "                        specify a sequence region. Got:\"\"\" + repr(start_region) + \"\"\" for \n",
    "                        sequence \"\"\" + repr(start_sequence))\n",
    "        \n",
    "    for i in seq_range:\n",
    "        residue = get_equivalentresidue(i + 1, start_sequence, end_sequence)\n",
    "        residue[0] = residue[0]-1\n",
    "        if residue[1] > 4:\n",
    "            residue.append(start_sequence[i])\n",
    "            residues.append(residue)\n",
    "    \n",
    "    #do this again in reverse\n",
    "    rev_residues = []\n",
    "    seq_reversed, new_region_start, new_region_end = reverse_sequence(start_sequence, start_region[0] + 2, start_region[1] + 2)\n",
    "    reverse_end_sequence = end_sequence[::-1]\n",
    "    for i in range(new_region_start, new_region_end + 1):\n",
    "        rev_residue = get_equivalentresidue(i + 1, seq_reversed, reverse_end_sequence)\n",
    "        if rev_residue[1] > 4:\n",
    "            rev_residue.append(seq_reversed[i])\n",
    "            rev_residues.append(rev_residue)\n",
    "    \n",
    "    #assign scores for every residue in end_sequence\n",
    "    end_sequence_scores = [0 for i in end_sequence]\n",
    "    end_sequence_flanking = [0 for i in end_sequence]\n",
    "        \n",
    "    #and in reverse\n",
    "    end_sequence_scores_reverse = [np.nan for i in reverse_end_sequence]\n",
    "    #reverse the reversed scores\n",
    "    for i in rev_residues:\n",
    "        i[0] = len(end_sequence_scores) - i[0] #restore the original residue number to each residue\n",
    "        end_sequence_scores_reverse[i[0]] = i[1] #add scores to every residue in the output\n",
    "    rev_residues = rev_residues[::-1] #reverses the rev_residues so its in the same order as residues\n",
    "    #print(residues)\n",
    "    \n",
    "    #combine forward and reverse residues\n",
    "    for residue in residues:\n",
    "        end_sequence_scores[residue[0]] = np.nan_to_num(residue[1])\n",
    "    for residue in rev_residues:\n",
    "        end_sequence_scores[residue[0]] += np.nan_to_num(residue[1])\n",
    "    \n",
    "    #find the range that the sequence confidently falls under\n",
    "    try:\n",
    "        max_score = max(end_sequence_scores)\n",
    "        first_max_index = False\n",
    "        last_max_index = 0\n",
    "        last_max_score = 0\n",
    "        for index, score in enumerate(end_sequence_scores):\n",
    "            if score > max_score * 0.7 and first_max_index == False: #begin the definite range where the score is 80% of the max score\n",
    "                first_max_index = index\n",
    "                first_max_score = score\n",
    "            if score > max_score * 0.7: #end the definite range where the score is 80% of the max score\n",
    "                last_max_index = index\n",
    "                last_max_score = score\n",
    "    except:\n",
    "        raise Exception(\"sequence range not found.\")\n",
    "    \n",
    "    if debug == True:\n",
    "        print(\"well defined bounds:\", first_max_index, last_max_index)\n",
    "        \n",
    "    # Give residues a score based on all the flanking scores in end_sequence\n",
    "    for residue in residues:\n",
    "        flanking_score = 0\n",
    "        for i in range(-5,6):\n",
    "            try:\n",
    "                new_score = end_sequence_scores[residue[0] + i]\n",
    "                if np.isnan(new_score) == False:\n",
    "                    flanking_score += new_score\n",
    "            except:\n",
    "                pass\n",
    "        end_sequence_flanking[residue[0]] = flanking_score\n",
    "        residue.append(flanking_score)\n",
    "    \n",
    "    start = first_max_index\n",
    "    end = last_max_index\n",
    "\n",
    "    score = 0\n",
    "    numbers = 0\n",
    "    for i in range(start, end + 1):\n",
    "        if np.isnan(end_sequence_scores[i]) == False:\n",
    "            score += end_sequence_scores[i]\n",
    "        numbers += 1\n",
    "    score = score/numbers\n",
    "    score = (score/22)*100\n",
    "    \n",
    "    return {\"start\" : start, \"end\": end, \"score\" : score}\n",
    "\n",
    "def int_or_nan(intended_integer):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    intended_integer : anything convertible to int\n",
    "        Input that should be converted to int.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int or float\n",
    "        Input integer or float if input couldn't be converted.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(intended_integer)\n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "def convert_regions(regions : dict, seq1 : str, seq2) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a dictionary of significant regions as an input, converts them from sequence 1\n",
    "    to sequence 2. If a dict of sequences is given for seq2, then will find the region \n",
    "    in all those sequences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regions : dict or str in dict format\n",
    "        Input set of regions to be converted.\n",
    "    seq1 : str\n",
    "        Sequence the regions are from.\n",
    "    seq2 : str or dict\n",
    "        Sequence the regions should be converted to.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        New dictionary of regions that have been converted from seq1 sequence to seq2 \n",
    "        sequence.\n",
    "\n",
    "    \"\"\"\n",
    "    #   Check that none of the inputs are null\n",
    "    if pd.isnull(regions) or pd.isnull(seq1) or pd.isnull(seq2):\n",
    "        return np.NaN\n",
    "    if isinstance(regions, str):\n",
    "        regions = ast.literal_eval(regions) # Convert the dictionary string to dict\n",
    "    assert isinstance(seq1, str), \"Expected str for seq1, got \" + repr(type(seq1))\n",
    "    \n",
    "    #   Check if seq2 is a dictionary, in which case interpret it as multiple sequences to \n",
    "    #   convert to.\n",
    "    multiple_target_sequences = False\n",
    "    if isinstance(seq2, dict):\n",
    "        multiple_target_sequences = True\n",
    "    else:\n",
    "        assert isinstance(seq2, str), \"Expected str or dict for seq2, got \" + repr(type(seq2))\n",
    "    \n",
    "    #   If seq2 is only one sequence, wrap it in a dictionary so it is dealt with the \n",
    "    #   same as if multiple sequences.\n",
    "    if multiple_target_sequences == False:\n",
    "        seq2 = {\"placeholder\" : seq2}\n",
    "    \n",
    "    #   Iterate through the target sequences.\n",
    "    new_regions_per_seq2 = {}\n",
    "    \n",
    "    for target_key in seq2:\n",
    "        #   Make a dictionary for storing the edited regions\n",
    "        new_regions = {}\n",
    "        \n",
    "        #   Iterate through the regions.\n",
    "        for region in regions:\n",
    "            #   Get all the dictionary keys of the regions\n",
    "            #print(\"Here we go\")\n",
    "            #print(regions[region])\n",
    "            keys = regions[region].copy().keys()\n",
    "            \n",
    "            #   Copy regions for editing\n",
    "            new_regions[region] = regions[region].copy()\n",
    "            \n",
    "            #   If just a position is offered, indicating 1 amino acid\n",
    "            if \"position\" in keys:\n",
    "                #   Convert that amino acid position\n",
    "                converted = convert_region(seq1, \n",
    "                                           int_or_nan(regions[region][\"position\"]), \n",
    "                                           seq2[target_key]).copy()\n",
    "                new_regions[region][\"position\"] = converted.copy()[\"start\"]\n",
    "                new_regions[region][\"score\"] = converted.copy()[\"score\"]\n",
    "                \n",
    "            #   If a begin and end is offered, so usually an amino acid range\n",
    "            if \"begin\" in keys and \"end\" in keys:\n",
    "                converted = convert_region(seq1, \n",
    "                                           [int_or_nan(regions[region][\"begin\"]), int_or_nan(regions[region][\"end\"])],\n",
    "                                           seq2[target_key])\n",
    "                new_regions[region][\"begin\"] = converted.copy()[\"start\"]\n",
    "                new_regions[region][\"end\"] = converted.copy()[\"end\"]\n",
    "                new_regions[region][\"score\"] = converted.copy()[\"score\"]\n",
    "                \n",
    "        #   Save this set of regions in into the dictionary of regions for each seq2\n",
    "        new_regions_per_seq2[target_key] = new_regions.copy()\n",
    "    \n",
    "    #   If seq2 was a string, take the output out of the dictionary it was wrapped in.\n",
    "    if multiple_target_sequences == False:\n",
    "        new_regions_per_seq2 = new_regions_per_seq2[\"placeholder\"]\n",
    "    \n",
    "    return new_regions_per_seq2\n",
    "\n",
    "def get_uniprot_sequence(unicode : str, pass_nan : bool = True, pass_no_output = True, debug : bool = True) -> str:\n",
    "    \"\"\"\n",
    "    When given a uniprot accession code will return the sequence of the protein.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unicode : str\n",
    "        Uniprot accession code.\n",
    "    pass_nan : bool, optional\n",
    "        As long as True, input of nan will be passed, otherwise will raise exception.\n",
    "    pass_no_output : bool, optional\n",
    "        If false, output of \"\" will raise exception.\n",
    "    debug : bool, optional\n",
    "        Should this function print as it goes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Seqeuence of protein to which uniprot accession was given.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check exceptions with the inputs\n",
    "    for i in [pass_nan, pass_no_output, debug]:\n",
    "        assert isinstance(i, bool), \"get_uniprot_sequence() options must be bools, found \" + repr(type(i))\n",
    "    if pass_nan == False and pd.isnull(unicode):\n",
    "        raise Exception(\"Given nan as unicode, which is not allowed if pass_nan == False.\")\n",
    "    if pd.isnull(unicode) == False:\n",
    "        assert isinstance(unicode, str), \"Expected string or nan for unicode, got '\" + repr(unicode) + \"', which is \" + repr(type(unicode))\n",
    "        \n",
    "        # Fetch the sequence of the accession given\n",
    "        if debug == True: print(\"getting\", \"https://rest.uniprot.org/uniprotkb/\" + unicode + \".fasta\")\n",
    "        soup = requests.get(\"https://rest.uniprot.org/uniprotkb/\" + unicode + \".fasta\").text\n",
    "        output = \"\".join(soup.split(\"\\n\")[1:-1])\n",
    "        \n",
    "        # If pass_no_output = False then raise an exception if the output is \"\"\n",
    "        if pass_no_output == False and output == \"\":\n",
    "            raise Exception(f\"No sequence found for {unicode} and pass_no_output is False.\")\n",
    "        \n",
    "        # Otherwise a message if output is \"\"\n",
    "        if output == \"\" and debug == True:\n",
    "            print(f\"No sequence found for {unicode}.\")\n",
    "\n",
    "        return output\n",
    "    \n",
    "    else: # If input was nan and pass_nan = True then print this message\n",
    "        if debug == True: print(\"Given nan as unicode to function get_uniprot_sequence().\")\n",
    "\n",
    "def plusminusorNaN(input, separator):\n",
    "    \"\"\"\n",
    "    This function splits a number with a combined deviation separated (e.g. by ) into a list \n",
    "    with two seperate numbers, or otherwise returns an extra NaN if there is no \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return input.split(separator)\n",
    "    except:\n",
    "        return [input, np.NaN]\n",
    "\n",
    "def separatevariance(input, separator, var_addon):\n",
    "    \"\"\"\n",
    "    Takes a pandas dataframe and separates (e.g. by ) variance into their own columns with a \n",
    "    custom header addon.\n",
    "    \"\"\"\n",
    "    data = input\n",
    "    tosplit = []\n",
    "    for col in data:\n",
    "        for bit in data[col]:\n",
    "            try:\n",
    "                if separator in bit:\n",
    "                    tosplit.append(col)\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    for col in tosplit:\n",
    "        data[col + var_addon] = data[col].apply(lambda x : plusminusorNaN(x, separator)[1])\n",
    "        data[col] = data[col].apply(lambda x : plusminusorNaN(x, separator)[0])\n",
    "    return data\n",
    "\n",
    "def get_oximouse_data(age : str):\n",
    "    \"\"\"\n",
    "    Fetch oximouse data as a dataframe. Age can be \"aged\", \"young\", or \"detected\".\n",
    "    Aged or young will return the data for the aged or young mice, respectively.\n",
    "    Detected returns a list of every cysteine that was detected with oximouse.\n",
    "    Oximouse data maps oxidation of specific cysteines in different body parts in\n",
    "    aged or young mice.\n",
    "    Reference:\n",
    "    Xiao, H., Jedrychowski, M. P., Schweppe, D. K., Huttlin, E. L., Yu, Q., Heppner, D. E., Li, J., Long, J., Mills, E. L., Szpyt, J., He, Z., Du, G., Garrity, R., Reddy, A., Vaites, L. P., Paulo, J. A., Zhang, T., Gray, N. S., Gygi, S. P., & Chouchani, E. T. (2020). A Quantitative Tissue-Specific Landscape of Protein Redox Regulation during Aging. Cell, 180(5), 968-983.e24. https://doi.org/10.1016/j.cell.2020.02.012\n",
    "    \"\"\"\n",
    "    if age == \"aged\":\n",
    "        df = pd.read_csv(\"https://piecole.com/data/oximouse_sensitive_old.txt\", sep = \"\\t\")\n",
    "    elif age == \"young\":\n",
    "        df = pd.read_csv(\"https://piecole.com/data/oximouse_sensitive_young.txt\", sep = \"\\t\")\n",
    "    elif age == \"detected\":\n",
    "        df = pd.read_csv(\"https://piecole.com/data/oximouse_all_cysteines_detected.txt\", sep = \"\\t\")\n",
    "    else:\n",
    "        raise Exception(\"age must be 'aged', 'young', or 'detected'. Given: \" + repr(age) + \".\")\n",
    "    #  Split data into measurement + variance\n",
    "    df = separatevariance(df, \"\", \" SEM\")\n",
    "\n",
    "    print(f\"Downloaded {age} oximouse data. Cite:\")\n",
    "    print(\"Xiao, H., Jedrychowski, M. P., Schweppe, D. K., Huttlin, E. L., Yu, Q., Heppner, D. E., Li, J., Long, J., Mills, E. L., Szpyt, J., He, Z., Du, G., Garrity, R., Reddy, A., Vaites, L. P., Paulo, J. A., Zhang, T., Gray, N. S., Gygi, S. P., & Chouchani, E. T. (2020). A Quantitative Tissue-Specific Landscape of Protein Redox Regulation during Aging. Cell, 180(5), 968-983.e24. https://doi.org/10.1016/j.cell.2020.02.012\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_alphafold_structure(uniprot_code : str, folder : str = \"structures\", extension = \"ent\", strict = False, debug = False):\n",
    "    \"\"\"\n",
    "    Downloads a structure from AlphaFold for a given uniprot code.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uniprot_code : str\n",
    "        Uniprot code of the protein to download.\n",
    "    folder : str, optional\n",
    "        Folder to save the structure to. The default is \"structures\".\n",
    "    extension : str, optional\n",
    "        Extension of the file. The default is \"ent\".\n",
    "    strict : bool, optional\n",
    "        Whether to raise an exception if the structure is not found. The default is False.\n",
    "    debug : bool, optional\n",
    "        Whether to print messages as it goes. The default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    folder = parse_folder(folder)\n",
    "\n",
    "    # Check whether the structure exists\n",
    "    if os.path.exists(folder + uniprot_code + \".\" + extension):\n",
    "        if debug:\n",
    "            print(\"Already have structure for \" + uniprot_code + \".\")\n",
    "        return\n",
    "\n",
    "    # Get the structure\n",
    "    print(\"Downloading structure for \" + uniprot_code + \" from AlphaFold. Please cite: \")\n",
    "    print(\"Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583589 (2021). https://doi.org/10.1038/s41586-021-03819-2\")\n",
    "    url = \"https://alphafold.ebi.ac.uk/files/AF-\" + uniprot_code + \"-F1-model_v4.pdb\"\n",
    "    data = requests.get(url, allow_redirects=True)\n",
    "\n",
    "    if \"NoSuchKey\" in str(data.content):\n",
    "        if strict:\n",
    "            raise Exception(\"AlphaFold structure for \" + uniprot_code + \" not found.\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"AlphaFold structure for \" + uniprot_code + \" not found.\")\n",
    "            return\n",
    "\n",
    "    # Save the structure\n",
    "    open(folder + uniprot_code + \".\" + extension, 'wb').write(data.content)\n",
    "\n",
    "def PDBsearch(query : str) -> list:\n",
    "    \"\"\"\n",
    "    Takes a query and returns a list of PDB IDs that match that query.\n",
    "    \"\"\"\n",
    "    #   Check input is a string\n",
    "    assert isinstance(query, str), \"Expected str for query, got \" + repr(type(query))\n",
    "    \n",
    "    # Make the query\n",
    "    query = PDBquery(query)\n",
    "    # Execute the query\n",
    "    results = set(query())\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_PDB_structure(pdb_id : str, folder : str = \"structures\", extension = \"ent\", strict = False, debug = False):\n",
    "    \"\"\"\n",
    "    Downloads a structure from the PDB for a given PDB ID.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdb_id : str\n",
    "        PDB ID of the structure to download.\n",
    "    folder : str, optional\n",
    "        Folder to save the structure to. The default is \"structures\".\n",
    "    extension : str, optional\n",
    "        Extension of the file. The default is \"ent\".\n",
    "    strict : bool, optional\n",
    "        Whether to raise an exception if the structure is not found. The default is False.\n",
    "    debug : bool, optional\n",
    "        Whether to print messages as it goes. The default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    folder = parse_folder(folder)\n",
    "\n",
    "    # Check whether the structure exists\n",
    "    if os.path.exists(folder + pdb_id + \".\" + extension):\n",
    "        if debug:\n",
    "            print(\"Already have structure for \" + pdb_id + \".\")\n",
    "        return\n",
    "\n",
    "    # Get the structure\n",
    "    print(\"Downloading structure for \" + pdb_id + \" from PDB.\")\n",
    "\n",
    "    url = \"https://files.rcsb.org/download/\" + pdb_id + \".pdb\"\n",
    "    data = requests.get(url, allow_redirects=True)\n",
    "\n",
    "    open(folder + pdb_id + \".\" + extension, 'wb').write(data.content)\n",
    "\n",
    "import propka.run as pk\n",
    "\n",
    "def run_propka(input_file, structure_folder = \"pdb\", structure_extension = \"ent\", propka_folder = \"propka/\", check = True):\n",
    "    \"\"\"\n",
    "    Checks if a propka file exists, if not then it attempts to make compute one\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    input_file : str  \n",
    "        The name of the file to compute propka for.\n",
    "    structure_folder : str, optional  \n",
    "        The folder to look for the structure in. The default is \"pdb\".\n",
    "    structure_extension : str, optional\n",
    "        The extension of the structure file. The default is \"ent\".\n",
    "    propka_folder : str, optional\n",
    "        The folder to save the propka file in. The default is \"propka/\".\n",
    "    check : bool, optional\n",
    "        Whether to check if the propka file exists before computing it. The default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    i : propka.run.single\n",
    "        The propka object. Also saves it to a file.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    worked = False\n",
    "\n",
    "    # Check if propka is installed\n",
    "    try:\n",
    "        pk\n",
    "    except:\n",
    "        raise Exception(\"PROPKA not installed. Please install to use this function.\")\n",
    "\n",
    "    propka_folder = parse_folder(propka_folder)\n",
    "    propka_path = propka_folder + \"pdb\" + input_file + \".pka\"\n",
    "    # Check if the propka file exists\n",
    "    if os.path.exists(propka_path) and check:\n",
    "        print(propka_path + f\" already exists at {propka_path}. If you want to recompute it, set check = False.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Going to comput propka for \" + input_file + \".\")\n",
    "        structure_folder = parse_folder(structure_folder)\n",
    "        try:\n",
    "            path = glob.glob(structure_folder + \"/**/\" + input_file + \"*.*\" + structure_extension + \"*\", recursive = True)[0]\n",
    "        except:\n",
    "            print(\"No structure found for '\" + input_file + \"'.\")\n",
    "            return\n",
    "        print(\"Calculating pKas with PROPKA and saving to file. Please cite:\")\n",
    "        print(\"Improved Treatment of Ligands and Coupling Effects in Empirical Calculation and Rationalization of pKa Values. Chresten R. Sndergaard, Mats H. M. Olsson, Micha Rostkowski, and Jan H. Jensen. Journal of Chemical Theory and Computation 2011 7 (7), 2284-2295. DOI: 10.1021/ct200133y\")\n",
    "\n",
    "        try:\n",
    "            if \"gz\" in path:\n",
    "                with gzip.open(path, \"rt\") as unzipped:\n",
    "                    i = pk.single(path.split(structure_extension)[0] + \"pdb\", optargs = [\"-q\"], stream = unzipped) #perform PROPKA on the file\n",
    "            else:\n",
    "                with open(path, \"rt\") as f:\n",
    "                    i = pk.single(path.split(structure_extension)[0] + \"pdb\", optargs = [\"-q\"], stream = f)\n",
    "            worked = True\n",
    "        except:\n",
    "            print(\"PROPKA failed for: \", input_file)\n",
    "            with open(\"PROPKA failed for.txt\", \"a\") as file:\n",
    "                file.write(input_file + \"\\r\")\n",
    "            \n",
    "    if worked == True:\n",
    "        # Move the file to propka folder \n",
    "        shutil.move(path.split(\"\\\\\")[-1].split(\"ent\")[0] + \"pka\", propka_path.replace(\"/\", \"/pdb\"))\n",
    "        return i\n",
    "\n",
    "# CYSTEINE SPECIFIC, MAKE NON CYSTEINE SPECIFIC\n",
    "def readpropka(filepath): #reads a propka file, saving the cysteines\n",
    "    found = False\n",
    "    try:\n",
    "        with open(filepath, mode = \"r\") as thisFile:\n",
    "            found = True\n",
    "            pkadatareader = csv.reader(thisFile, delimiter = \" \")\n",
    "            pkadata = []\n",
    "            for pkaline in pkadatareader:\n",
    "                while '' in pkaline:\n",
    "                    pkaline.remove('')\n",
    "                pkadata.append(pkaline)\n",
    "    except:\n",
    "        print(filepath, \"not found\")\n",
    "    \n",
    "    if found == True:\n",
    "        # Save the cysteine rows as a list\n",
    "        # Previously didn't work with 4-digit residue numbers or more, so changed it.\n",
    "        cysteines = [i for i in [a for a in pkadata if len(a) > 5] if i[0][0:3] == \"CYS\"]\n",
    "\n",
    "        # Deal with the stupid formatting which means some bits of data join onto each other,\n",
    "        # by splittting these bits of data in two\n",
    "        for cysteine in cysteines:\n",
    "            \n",
    "            # Separate the residue number from the residue name in cases where residue\n",
    "            # number was digits or more.\n",
    "            if len(cysteine[0]) > 3:\n",
    "                cysteine.insert(1, re.split(\"(\\d+)\", cysteine[0])[0])\n",
    "                cysteine.insert(2, re.split(\"(\\d+)\", cysteine[0])[1])\n",
    "                cysteine.pop(0)            \n",
    "\n",
    "            if len(cysteine) < 16:\n",
    "                for i in range(7):\n",
    "                    cysteine.insert(3, '') # add extra spaces to the bond rows\n",
    "            if len(cysteine) > 13:\n",
    "                if(len(cysteine[11]) > 3):\n",
    "                    cysteine.insert(12, re.split(\"(\\d+)\", cysteine[11])[0])\n",
    "                    cysteine.insert(13, re.split(\"(\\d+)\", cysteine[11])[1])\n",
    "                    cysteine.pop(11)\n",
    "                if(len(cysteine[15]) > 3):\n",
    "                    cysteine.insert(16, re.split(\"(\\d+)\", cysteine[15])[0])\n",
    "                    cysteine.insert(17, re.split(\"(\\d+)\", cysteine[15])[1])\n",
    "                    cysteine.pop(15)\n",
    "                if(len(cysteine[19]) > 3):\n",
    "                    cysteine.insert(20, re.split(\"(\\d+)\", cysteine[19])[0])\n",
    "                    cysteine.insert(21, re.split(\"(\\d+)\", cysteine[19])[1])\n",
    "                    cysteine.pop(19)\n",
    "        \n",
    "        #remove the bond rows (only keep the actual cysteine)\n",
    "        cysteines = [i[:10] for i in cysteines if i[3] != \"\"]\n",
    "        \n",
    "        #turn cysteines into a pandas dataframe\n",
    "        save_columns = [\"resn\", \"resi\", \"chain\", \"pka\", \"buried\", \"nil\", \"desolvation regular 1\", \"desolvation regular 2\", \"effects re 1\", \"effects re 2\"]\n",
    "        data = pd.DataFrame(columns = save_columns, dtype = object)\n",
    "        for i in cysteines:\n",
    "            # Turn list into a dataframe column\n",
    "            newline = pd.DataFrame([i], columns = save_columns)\n",
    "            data = pd.concat([data, newline],\n",
    "                             ignore_index = True)\n",
    "            \n",
    "        #add the PDBid as a column\n",
    "        data[\"PDBid\"] = filepath.split(\"pdb\")[-1].split(\".pka\")[0]\n",
    "        \n",
    "        #drop nil\n",
    "        data = data.drop(columns = [\"nil\", \"resn\"])\n",
    "        \n",
    "        return data\n",
    "\n",
    "# MAKING NON CYSTEINE SPECIFIC\n",
    "def readpropka_better(filepath):\n",
    "    \"\"\"\n",
    "    Reads a propka file, extracting PROPKA info about each residue.\n",
    "    CAN THIS DEAL WITH NMR? CAN PROPKA NOT DEAL WITH NMR?\n",
    "    \"\"\"\n",
    "    found = False\n",
    "    try:\n",
    "        with open(filepath, mode = \"r\") as thisFile:\n",
    "            found = True\n",
    "            pkadatareader = csv.reader(thisFile, delimiter = \" \")\n",
    "            pkadata = []\n",
    "            for pkaline in pkadatareader:\n",
    "                while '' in pkaline:\n",
    "                    pkaline.remove('')\n",
    "                pkadata.append(pkaline)\n",
    "    except:\n",
    "        print(filepath, \"not found\")\n",
    "    \n",
    "    if found == True:\n",
    "        # Save the residues rows as a list\n",
    "        # Previously didn't work with 4-digit residue numbers or more,\n",
    "        # so changed it.\n",
    "        residues = [i for i in [a for a in pkadata if len(a) > 5]]\n",
    "\n",
    "        # Deal with the stupid formatting which means some bits of\n",
    "        # data join onto each other, by splittting these bits of data in two\n",
    "        for residue in residues:\n",
    "            \n",
    "            # Separate the residue number from the residue name in cases where residue\n",
    "            # number was digits or more.\n",
    "            if len(residue[0]) > 3:\n",
    "                residue.insert(1, re.split(\"(\\d+)\", residue[0])[0])\n",
    "                residue.insert(2, re.split(\"(\\d+)\", residue[0])[1])\n",
    "                residue.pop(0)            \n",
    "\n",
    "            if len(residue) < 16:\n",
    "                for i in range(7):\n",
    "                    residue.insert(3, '') # add extra spaces to the bond rows\n",
    "            if len(residue) > 13:\n",
    "                if(len(residue[11]) > 3):\n",
    "                    residue.insert(12, re.split(\"(\\d+)\", residue[11])[0])\n",
    "                    residue.insert(13, re.split(\"(\\d+)\", residue[11])[1])\n",
    "                    residue.pop(11)\n",
    "                if(len(residue[15]) > 3):\n",
    "                    residue.insert(16, re.split(\"(\\d+)\", residue[15])[0])\n",
    "                    residue.insert(17, re.split(\"(\\d+)\", residue[15])[1])\n",
    "                    residue.pop(15)\n",
    "                if(len(residue[19]) > 3):\n",
    "                    residue.insert(20, re.split(\"(\\d+)\", residue[19])[0])\n",
    "                    residue.insert(21, re.split(\"(\\d+)\", residue[19])[1])\n",
    "                    residue.pop(19)\n",
    "        \n",
    "        # Remove the bond rows (only keep the actual residue)\n",
    "        residues = [i[:10] for i in residues if i[3] != \"\"]\n",
    "        \n",
    "        # Turn list of residues into a pandas dataframe\n",
    "        save_columns = [\"resn\",\n",
    "                        \"resi\",\n",
    "                        \"chain\",\n",
    "                        \"pka\",\n",
    "                        \"buried\",\n",
    "                        \"nil\",\n",
    "                        \"desolvation regular 1\",\n",
    "                        \"desolvation regular 2\",\n",
    "                        \"effects re 1\",\n",
    "                        \"effects re 2\"]\n",
    "        data = pd.DataFrame(columns = save_columns, dtype = object)\n",
    "        for i in residues:\n",
    "            newline = pd.DataFrame([i], columns = save_columns)\n",
    "            data = pd.concat([data, newline],\n",
    "                             ignore_index = True)\n",
    "            \n",
    "        # Add the PDBid as a column\n",
    "        data[\"PDBid\"] = filepath.split(\"pdb\")[-1].split(\".pka\")[0]\n",
    "        \n",
    "        # Drop nil\n",
    "        data = data.drop(columns = [\"nil\", \"resn\"])\n",
    "        \n",
    "        return data\n",
    "\n",
    "def check_structure_for_proximal_atoms(structure,\n",
    "                                       residue_1,\n",
    "                                       residue_2,\n",
    "                                       atom_1 = \"CA\",\n",
    "                                       atom_2 = \"CA\",\n",
    "                                       max_distance = 10):\n",
    "    \"\"\"\n",
    "    Open a protein structure (or structure) and search for two\n",
    "    residues that are within a specified distance of each other. Returning a list of\n",
    "    dictionaries with two residues and their distance from each other.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    structure_file : str or  Bio.PDB.Structure\n",
    "        Structure or path to the structure file\n",
    "    residue_1 : str\n",
    "        Residue three letter code of the first residue\n",
    "    residue_2 : str\n",
    "        Residue three letter code of the second residue\n",
    "    atom_1 : str, optional\n",
    "        Name of the atom in the first residue. Default is \"CA\"\n",
    "    atom_2 : str, optional\n",
    "        Name of the atom in the second residue. Default is \"CA\"\n",
    "    distance : int, optional\n",
    "        Distance cutoff. Default is 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List  \n",
    "        List of dictionaries containing the two residues and their distance from each other.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    check_structure_for_proximal_atoms(\"structures/A2A5R2.ent\", \"CYS\", \"CYS\", atom_1 = \"SG\", atom_2 = \"SG\", max_distance = 5)    \"\"\"\n",
    "\n",
    "    if isinstance(structure, str):\n",
    "        # Make sure structure file exists\n",
    "        assert os.path.exists(structure), \"Structure file not found.\"\n",
    "        structures = PDBParser().get_structure(\"structure\", structure)\n",
    "    else:\n",
    "        # Make sure structure is a structure\n",
    "        assert isinstance(structure, Structure.Structure), \"Expected Bio.PDB.Structure.Structure for structure, got \" + repr(type(structure))\n",
    "        structures = structure\n",
    "    output_residues = []\n",
    "    for structure in structures:\n",
    "        # Compile all the residues, so that intermolecular interactions can be\n",
    "        # checked\n",
    "        residues = []\n",
    "        for chain in structure:\n",
    "            for residue in chain:\n",
    "                # Only keep relevent residues\n",
    "                if residue.get_resname() == residue_1 or residue.get_resname() == residue_2:\n",
    "                    residue.chain = chain\n",
    "                    residues.append(residue)\n",
    "        # Iterate through and measure the distance between all the residues\n",
    "        for residue_A in residues:\n",
    "            for residue_B in residues:\n",
    "                # Check the residues are different\n",
    "                if residue_A != residue_B:\n",
    "                    # Check the atoms are in the residues\n",
    "                    assert atom_1 in [atom.get_id() for atom in residue_A], f\"Atom {atom_1} not found in residue {residue_A.get_resname()}{residue_A.get_id()[1]}\"\n",
    "                    assert atom_2 in [atom.get_id() for atom in residue_B], f\"Atom {atom_2} not found in residue {residue_B.get_resname()}{residue_B.get_id()[1]}\"\n",
    "                    # Measure the distance between the atoms and document if it is\n",
    "                    distance = residue_A[atom_1] - residue_B[atom_2]\n",
    "                    if distance < max_distance:\n",
    "                        output_dict = {\"residue number A\" : residue_A.id[1],\n",
    "                                       \"chain A\" : residue_A.chain.id,\n",
    "                                        \"residue number B\" : residue_B.id[1],\n",
    "                                        \"chain B\" : residue_B.chain.id,\n",
    "                                        \"distance\" : distance}\n",
    "                        output_residues.append(output_dict)\n",
    "            # Remove residue_A from residues so it wont get tested again\n",
    "            residues.remove(residue_A)\n",
    "    return output_residues\n",
    "\n",
    "class Sequence:\n",
    "    \"\"\"\n",
    "    Class for storing and manipulating sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequence = \"\", **kwargs):\n",
    "        self.sequence = sequence\n",
    "        if \"sequence_type\" in kwargs:\n",
    "            assert kwargs[\"sequence_type\"] in [\"protein\", \"DNA\", \"RNA\"], \"Expected 'protein', 'DNA', or 'RNA' for sequence_type, got \" + repr(kwargs[\"sequence_type\"])\n",
    "            self.sequence_type = kwargs[\"sequence_type\"]\n",
    "        if \"name\" in kwargs:\n",
    "            self.name = kwargs[\"name\"]\n",
    "    \n",
    "    # String behaviour of sequence\n",
    "    def __str__(self):\n",
    "        return self.sequence\n",
    "    # Return behaviour\n",
    "    def __repr__(self):\n",
    "        return self.sequence\n",
    "    # Len behaviour\n",
    "    def __len__(self):\n",
    "        return len(self.sequence)\n",
    "    # Subscriptable\n",
    "    def __getitem__(self, key):\n",
    "        return self.sequence[key]\n",
    "    # Adding together behaviour\n",
    "    def __add__(self, other):\n",
    "        # Check that the sequence_types are the same\n",
    "        if hasattr(self, \"sequence_type\") and hasattr(other, \"sequence_type\"):\n",
    "            assert self.sequence_type == other.sequence_type, \"Sequence types do not match. \" + repr(self.sequence_type) + \" and \" + repr(other.sequence_type)\n",
    "        sequence = Sequence(self.sequence + other.sequence)\n",
    "        if hasattr(self, \"sequence_type\"):\n",
    "            sequence.sequence_type = self.sequence_type\n",
    "        return sequence\n",
    "    \n",
    "    # Compair behaviour\n",
    "    def __eq__(self, other):\n",
    "        assert isinstance(other, Sequence), \"Expected Sequence for other, got \" + repr(type(other))\n",
    "        # If both have sequence types and they do not match, an error will be thrown\n",
    "        if hasattr(self, \"sequence_type\") and hasattr(other, \"sequence_type\"):\n",
    "            assert self.sequence_type == other.sequence_type, \"Sequence types do not match. \" + repr(self.sequence_type) + \" and \" + repr(other.sequence_type)\n",
    "        return self.sequence == other.sequence\n",
    "\n",
    "    def reverse(self):\n",
    "        \"\"\"\n",
    "        Reverse the sequence.\n",
    "        \"\"\"\n",
    "        self.sequence = self.sequence[::-1]\n",
    "\n",
    "    def search_for(self, search_sequence, region = None):\n",
    "        \"\"\"\n",
    "        Search for a sequence within the sequence.\n",
    "        \"\"\"\n",
    "        if region == None:\n",
    "            region = [1, len(self.sequence)]\n",
    "\n",
    "        if isinstance(search_sequence, Sequence):\n",
    "            search_sequence = str(search_sequence)\n",
    "\n",
    "        print(\"search_sequence\", search_sequence)\n",
    "        print(\"region\", region)\n",
    "        print(\"end_sequence\", self.sequence)\n",
    "\n",
    "        # THIS FUNCTION ASSUMES PROTEIN?\n",
    "        return convert_region(start_sequence = search_sequence,\n",
    "                              start_region = region,\n",
    "                              end_sequence = self.sequence)\n",
    "    \n",
    "    def transcribe(self):\n",
    "        \"\"\"\n",
    "        Transcribe DNA to RNA.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"sequence_type\"):\n",
    "            if self.sequence_type == \"DNA\":\n",
    "                self.sequence = self.sequence.replace(\"T\", \"U\")\n",
    "                self.sequence_type = \"RNA\"\n",
    "            else:\n",
    "                raise Exception(\"Sequence is not DNA, so cannot be transcribed.\")\n",
    "        else:\n",
    "            self.sequence = self.sequence.replace(\"T\", \"U\")\n",
    "            self.sequence_type = \"RNA\"\n",
    "\n",
    "    def blast(self):\n",
    "        assert hasattr(self, \"sequence_type\"), \"Sequence type must be defined for BLAST search.\"\n",
    "        \n",
    "        if self.sequence_type == \"DNA\" or self.sequence_type == \"RNA\":\n",
    "            result_handle = NCBIWWW.qblast(\"blastn\", \"nt\", self.sequence)\n",
    "        elif self.sequence_type == \"protein\":\n",
    "            result_handle = NCBIWWW.qblast(\"blastp\", \"nr\", self.sequence)\n",
    "        else:\n",
    "            raise Exception(\"Sequence type must be 'DNA', 'RNA', or 'protein'. Got \" + repr(self.sequence_type))\n",
    "        return NCBIXML.read(result_handle)\n",
    "\n",
    "    # Future functions:\n",
    "        # reverse_complement\n",
    "        # get_structure (alphafold and or PDB)\n",
    "        # could put all the sequence functions in this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDBsearch(\"PKG1a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7SSB'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDBsearch(\"PKG1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
